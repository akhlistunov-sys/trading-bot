# news_fetcher.py - –ü–û–õ–ù–´–ô –û–ë–ù–û–í–õ–ï–ù–ù–´–ô –° –ù–û–í–´–ú–ò –ò–°–¢–û–ß–ù–ò–ö–ê–ú–ò
import os
import aiohttp
import asyncio
import logging
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import json

logger = logging.getLogger(__name__)

class NewsFetcher:
    """–°–±–æ—Ä—â–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –∏ –Ω–æ–≤—ã–º–∏ API"""
    
    def __init__(self):
        # API –∫–ª—é—á–∏
        self.newsapi_key = os.getenv("NewsAPI")  # a9e56dc34399435e84a0492db880fbbf
        self.mediastack_key = os.getenv("mediastackAPI")  # 661a4c645e9a74be9bc6343c639eba1d
        self.zenserp_key = os.getenv("ZENSEPTAPI")  # d7207660-d210-11f0-9fa2-b34c09889c77
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (—Ä–∞–±–æ—Ç–∞—é—â–∏–µ)
        self.rss_feeds = {
            "investing_russia": "https://ru.investing.com/rss/news.rss",
            "finam_news": "https://www.finam.ru/international/analysis/conews/rsspoint/",
            "quote_rbc": "https://quote.rbc.ru/rss/news.rss",
            "banki_news": "https://www.banki.ru/xml/news.rss",
            "moex_simple": "https://www.moex.com/export/news.aspx?lang=ru"
        }
        
        # –§–∏–ª—å—Ç—Ä –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –î–õ–Ø –§–ò–ù–ê–ù–°–û–í–´–• –Ω–æ–≤–æ—Å—Ç–µ–π
        self.financial_keywords = [
            # –†—É—Å—Å–∫–∏–µ
            '–∞–∫—Ü–∏', '–∞–∫—Ü–∏–π', '–∞–∫—Ü–∏—è–º–∏', '–¥–∏–≤–∏–¥–µ–Ω–¥', '–¥–∏–≤–∏–¥–µ–Ω–¥—ã',
            '–æ—Ç—á–µ—Ç', '–∫–≤–∞—Ä—Ç–∞–ª', '–ø—Ä–∏–±—ã–ª—å', '–≤—ã—Ä—É—á–∫–∞', '—É–±—ã—Ç–æ–∫',
            '—Å–±–µ—Ä–±–∞–Ω–∫', '–≥–∞–∑–ø—Ä–æ–º', '–ª—É–∫–æ–π–ª', '–Ω–æ—Ä–Ω–∏–∫–µ–ª—å', '—Ä–æ—Å—Ç–µ–ª–µ–∫–æ–º',
            '–º–æ—Å–±–∏—Ä–∂–∞', '–≤—Ç–±', '—Ç–∏–Ω—å–∫–æ—Ñ—Ñ', '—è–Ω–¥–µ–∫—Å', '–æ–∑–æ–Ω',
            '–±–∏—Ä–∂', '–∫–æ—Ç–∏—Ä–æ–≤–∫', '–∏–Ω–≤–µ—Å—Ç', '—Ç—Ä–µ–π–¥', '–ø–æ—Ä—Ç—Ñ–µ–ª',
            '—Ä—É–±–ª', '–¥–æ–ª–ª–∞—Ä', '–µ–≤—Ä–æ', '–Ω–µ—Ñ—Ç', '–≥–∞–∑', '–∑–æ–ª–æ—Ç',
            '—ç–º–∏—Å—Å–∏', '–æ–±–ª–∏–≥–∞—Ü', '—Ñ–æ–Ω–¥', '—Ä—ã–Ω–æ–∫', '—ç–∫–æ–Ω–æ–º–∏–∫',
            '—Å–∞–Ω–∫—Ü', '—Ä–µ–≥—É–ª—è—Ç–æ—Ä', '—Ü–±', '–º–∏–Ω—Ñ–∏–Ω', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤',
            '–ø–æ–∫—É–ø–∫', '–ø—Ä–æ–¥–∞–∂', '—Å–¥–µ–ª–∫–∞', '—Å–ª–∏—è–Ω–∏', '–ø–æ–≥–ª–æ—â–µ–Ω',
            '—Ä–µ–∫–æ–º–µ–Ω–¥—É', '–∞–Ω–∞–ª–∏—Ç–∏–∫', '–ø—Ä–æ–≥–Ω–æ–∑', '–æ–∂–∏–¥–∞–Ω', '—Ü–µ–ª–µ–≤',
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ (–¥–ª—è investing.com)
            'stock', 'share', 'dividend', 'earnings', 'profit',
            'revenue', 'quarter', 'financial', 'deal', 'merger',
            'acquisition', 'growth', 'decline', 'bank', 'company',
            'market', 'exchange', 'invest', 'trade', 'russian',
            'moscow', 'moex', 'sberbank', 'gazprom', 'lukoil'
        ]
        
        logger.info("üì∞ NewsFetcher –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –Ω–æ–≤—ã–º–∏ API")
        logger.info(f"üîë NewsAPI: {'‚úÖ' if self.newsapi_key else '‚ùå'}")
        logger.info(f"üîë MediaStack: {'‚úÖ' if self.mediastack_key else '‚ùå'}")
        logger.info(f"üîë RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(self.rss_feeds)}")
    
    def _is_financial_news(self, text: str) -> bool:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¢–û–õ–¨–ö–û —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in self.financial_keywords)
    
    async def fetch_rss_feed(self, url: str, source_name: str) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        articles = []
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=10, ssl=False) as response:
                    if response.status == 200:
                        xml_content = await response.text()
                        
                        try:
                            root = ET.fromstring(xml_content)
                        except:
                            # –ü–æ–ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–æ–π –ø–∞—Ä—Å–µ—Ä –µ—Å–ª–∏ XML –±–∏—Ç—ã–π
                            return articles
                        
                        for item in root.findall('.//item'):
                            title_elem = item.find('title')
                            description_elem = item.find('description')
                            link_elem = item.find('link')
                            pub_date_elem = item.find('pubDate')
                            
                            if title_elem is not None:
                                title = title_elem.text or ''
                                description = description_elem.text if description_elem is not None else ''
                                
                                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                                full_text = f"{title} {description}".lower()
                                
                                # –§–∏–ª—å—Ç—Ä—É–µ–º –¢–û–õ–¨–ö–û —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
                                if self._is_financial_news(full_text):
                                    articles.append({
                                        'id': f"{source_name}_{pub_date_elem.text if pub_date_elem else ''}_{len(articles)}",
                                        'source': source_name,
                                        'title': title,
                                        'description': description,
                                        'content': description,
                                        'url': link_elem.text if link_elem is not None else '',
                                        'published_at': pub_date_elem.text if pub_date_elem else '',
                                        'source_name': source_name,
                                        'fetched_at': datetime.now().isoformat(),
                                        'is_financial': True
                                    })
                        
                        logger.debug(f"   ‚úÖ {source_name}: {len(articles)} —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
            
        except asyncio.TimeoutError:
            logger.warning(f"‚è∞ {source_name}: —Ç–∞–π–º–∞—É—Ç")
        except Exception as e:
            logger.debug(f"üîß {source_name} –æ—à–∏–±–∫–∞: {str(e)[:50]}")
        
        return articles
    
    async def fetch_newsapi(self) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ NewsAPI (–Ω–æ–≤–æ–µ!)"""
        if not self.newsapi_key:
            logger.debug("‚ö†Ô∏è NewsAPI –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            return []
        
        articles = []
        
        try:
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
            params = {
                'apiKey': self.newsapi_key,
                'q': '(Russia OR Russian OR Moscow) AND (stocks OR shares OR market OR finance OR investment)',
                'language': 'en',
                'sortBy': 'publishedAt',
                'pageSize': 20,
                'from': (datetime.now() - timedelta(days=2)).strftime('%Y-%m-%d')
            }
            
            url = "https://newsapi.org/v2/everything"
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, timeout=10) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        if data.get('status') == 'ok':
                            for article in data.get('articles', []):
                                title = article.get('title', '')
                                description = article.get('description', '')
                                content = article.get('content', '')
                                
                                full_text = f"{title} {description} {content}".lower()
                                
                                if self._is_financial_news(full_text):
                                    articles.append({
                                        'id': f"newsapi_{article.get('publishedAt', '')}_{len(articles)}",
                                        'source': 'newsapi',
                                        'title': title,
                                        'description': description,
                                        'content': content or description,
                                        'url': article.get('url', ''),
                                        'published_at': article.get('publishedAt', ''),
                                        'source_name': article.get('source', {}).get('name', 'newsapi'),
                                        'fetched_at': datetime.now().isoformat(),
                                        'is_financial': True,
                                        'api_source': 'newsapi'
                                    })
                            
                            logger.info(f"‚úÖ NewsAPI: {len(articles)} —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
                        else:
                            logger.warning(f"‚ö†Ô∏è NewsAPI –æ—à–∏–±–∫–∞: {data.get('message', '')}")
                    else:
                        logger.warning(f"‚ö†Ô∏è NewsAPI HTTP –æ—à–∏–±–∫–∞: {response.status}")
            
        except asyncio.TimeoutError:
            logger.warning("‚è∞ NewsAPI: —Ç–∞–π–º–∞—É—Ç")
        except Exception as e:
            logger.error(f"‚ùå NewsAPI –æ—à–∏–±–∫–∞: {str(e)[:50]}")
        
        return articles
    
    async def fetch_mediastack(self) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ MediaStack (–Ω–æ–≤–æ–µ!)"""
        if not self.mediastack_key:
            logger.debug("‚ö†Ô∏è MediaStack –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            return []
        
        articles = []
        
        try:
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
            params = {
                'access_key': self.mediastack_key,
                'keywords': 'finance,stocks,market,investment',
                'countries': 'ru',
                'languages': 'en,ru',
                'limit': 15,
                'sort': 'published_desc'
            }
            
            url = "http://api.mediastack.com/v1/news"
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, timeout=10) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        if data.get('data'):
                            for article in data.get('data', []):
                                title = article.get('title', '')
                                description = article.get('description', '')
                                
                                full_text = f"{title} {description}".lower()
                                
                                if self._is_financial_news(full_text):
                                    articles.append({
                                        'id': f"mediastack_{article.get('published_at', '')}_{len(articles)}",
                                        'source': 'mediastack',
                                        'title': title,
                                        'description': description,
                                        'content': description,
                                        'url': article.get('url', ''),
                                        'published_at': article.get('published_at', ''),
                                        'source_name': article.get('source', 'mediastack'),
                                        'fetched_at': datetime.now().isoformat(),
                                        'is_financial': True,
                                        'api_source': 'mediastack'
                                    })
                            
                            logger.info(f"‚úÖ MediaStack: {len(articles)} —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
                        else:
                            logger.warning(f"‚ö†Ô∏è MediaStack –æ—à–∏–±–∫–∞: {data.get('error', {}).get('message', '')}")
                    else:
                        logger.warning(f"‚ö†Ô∏è MediaStack HTTP –æ—à–∏–±–∫–∞: {response.status}")
            
        except asyncio.TimeoutError:
            logger.warning("‚è∞ MediaStack: —Ç–∞–π–º–∞—É—Ç")
        except Exception as e:
            logger.error(f"‚ùå MediaStack –æ—à–∏–±–∫–∞: {str(e)[:50]}")
        
        return articles
    
    async def fetch_all_news(self) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –í–°–ï–• –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –í–°–ï–• –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
        logger.info("üì• –°–±–æ—Ä —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
        
        all_articles = []
        
        # 1. RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (—Ä–∞–±–æ—Ç–∞—é—â–∏–µ)
        rss_tasks = []
        for source_name, url in self.rss_feeds.items():
            rss_tasks.append(self.fetch_rss_feed(url, source_name))
        
        rss_results = await asyncio.gather(*rss_tasks, return_exceptions=True)
        
        for i, result in enumerate(rss_results):
            source_name = list(self.rss_feeds.keys())[i]
            if isinstance(result, list):
                all_articles.extend(result)
                logger.info(f"   üìä {source_name}: {len(result)} —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
            elif isinstance(result, Exception):
                logger.warning(f"   ‚ö†Ô∏è {source_name}: –æ—à–∏–±–∫–∞")
        
        # 2. NewsAPI (–µ—Å–ª–∏ –µ—Å—Ç—å –∫–ª—é—á)
        if self.newsapi_key:
            newsapi_articles = await self.fetch_newsapi()
            all_articles.extend(newsapi_articles)
        
        # 3. MediaStack (–µ—Å–ª–∏ –µ—Å—Ç—å –∫–ª—é—á)
        if self.mediastack_key:
            mediastack_articles = await self.fetch_mediastack()
            all_articles.extend(mediastack_articles)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
        unique_articles = []
        seen_titles = set()
        
        for article in all_articles:
            title_key = article['title'][:80].lower().strip()
            if title_key and title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_articles.append(article)
        
        removed = len(all_articles) - len(unique_articles)
        if removed > 0:
            logger.info(f"üîÑ –£–¥–∞–ª–µ–Ω–æ {removed} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–Ω–æ–≤—ã–µ —Å–Ω–∞—á–∞–ª–∞)
        unique_articles.sort(
            key=lambda x: x.get('published_at', ''),
            reverse=True
        )
        
        logger.info(f"üìä –ò–¢–û–ì–û: {len(unique_articles)} —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        logger.info(f"   üì∞ –ò—Å—Ç–æ—á–Ω–∏–∫–∏: RSS, {'NewsAPI, ' if self.newsapi_key else ''}{'MediaStack' if self.mediastack_key else ''}")
        
        return unique_articles[:30]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 30 –Ω–æ–≤–æ—Å—Ç—è–º–∏
    
    def get_source_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º"""
        return {
            'rss_feeds_count': len(self.rss_feeds),
            'newsapi_configured': bool(self.newsapi_key),
            'mediastack_configured': bool(self.mediastack_key),
            'zenserp_configured': bool(self.zenserp_key),
            'financial_keywords': len(self.financial_keywords),
            'total_sources': len(self.rss_feeds) + (1 if self.newsapi_key else 0) + (1 if self.mediastack_key else 0)
        }
