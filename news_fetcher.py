import os
import aiohttp
import asyncio
import logging
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import json

logger = logging.getLogger(__name__)

class NewsFetcher:
    """–°–±–æ—Ä—â–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
    
    def __init__(self):
        self.mediastack_key = os.getenv("mediastackAPI")
        self.zenserp_key = os.getenv("ZENSEPTAPI")
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        self.rss_feeds = {
            # –§–ò–ù–ê–ù–°–û–í–´–ï –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            "moex_main": "https://www.moex.com/export/news.aspx?lang=ru&cat=1",
            "moex_news": "https://www.moex.com/export/news.aspx?lang=ru&cat=100",
            "rbc_finance": "https://rssexport.rbc.ru/rbcnews/news/20/full.rss",
            "rbc_economics": "https://rssexport.rbc.ru/rbcnews/news/2/full.rss",
            "investing_russia": "https://ru.investing.com/rss/news.rss",
            # –î–û–ë–ê–í–õ–ï–ù–û: –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –±–ª–æ–≥–∏
            "banki_news": "https://www.banki.ru/xml/news.rss",
            "quote_russia": "https://quote.rbc.ru/rss/news.rss"
        }
        
        # –§–∏–ª—å—Ç—Ä –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –î–õ–Ø –§–ò–ù–ê–ù–°–û–í–´–• –Ω–æ–≤–æ—Å—Ç–µ–π
        self.financial_keywords = [
            '–∞–∫—Ü–∏', '–∞–∫—Ü–∏–π', '–∞–∫—Ü–∏—è–º–∏', '–¥–∏–≤–∏–¥–µ–Ω–¥', '–¥–∏–≤–∏–¥–µ–Ω–¥—ã',
            '–æ—Ç—á–µ—Ç', '–∫–≤–∞—Ä—Ç–∞–ª', '–ø—Ä–∏–±—ã–ª—å', '–≤—ã—Ä—É—á–∫–∞', '—É–±—ã—Ç–æ–∫',
            '—Å–±–µ—Ä–±–∞–Ω–∫', '–≥–∞–∑–ø—Ä–æ–º', '–ª—É–∫–æ–π–ª', '–Ω–æ—Ä–Ω–∏–∫–µ–ª—å', '—Ä–æ—Å—Ç–µ–ª–µ–∫–æ–º',
            '–º–æ—Å–±–∏—Ä–∂–∞', '–≤—Ç–±', '—Ç–∏–Ω—å–∫–æ—Ñ—Ñ', '—è–Ω–¥–µ–∫—Å', '–æ–∑–æ–Ω',
            '–±–∏—Ä–∂', '–∫–æ—Ç–∏—Ä–æ–≤–∫', '–∏–Ω–≤–µ—Å—Ç', '—Ç—Ä–µ–π–¥', '–ø–æ—Ä—Ç—Ñ–µ–ª',
            '—Ä—É–±–ª', '–¥–æ–ª–ª–∞—Ä', '–µ–≤—Ä–æ', '–Ω–µ—Ñ—Ç', '–≥–∞–∑', '–∑–æ–ª–æ—Ç',
            '—ç–º–∏—Å—Å–∏', '–æ–±–ª–∏–≥–∞—Ü', '—Ñ–æ–Ω–¥', '—Ä—ã–Ω–æ–∫', '—ç–∫–æ–Ω–æ–º–∏–∫',
            '—Å–∞–Ω–∫—Ü', '—Ä–µ–≥—É–ª—è—Ç–æ—Ä', '—Ü–±', '–º–∏–Ω—Ñ–∏–Ω', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤',
            '–ø–æ–∫—É–ø–∫', '–ø—Ä–æ–¥–∞–∂', '—Å–¥–µ–ª–∫–∞', '—Å–ª–∏—è–Ω–∏', '–ø–æ–≥–ª–æ—â–µ–Ω',
            '—Ä–µ–∫–æ–º–µ–Ω–¥—É', '–∞–Ω–∞–ª–∏—Ç–∏–∫', '–ø—Ä–æ–≥–Ω–æ–∑', '–æ–∂–∏–¥–∞–Ω', '—Ü–µ–ª–µ–≤'
        ]
        
        logger.info("üì∞ NewsFetcher –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        logger.info(f"üîë –§–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(self.rss_feeds)}")
    
    def _is_financial_news(self, text: str) -> bool:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¢–û–õ–¨–ö–û —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in self.financial_keywords)
    
    async def fetch_rss_feed(self, url: str, source_name: str) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        articles = []
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=10, ssl=False) as response:
                    if response.status == 200:
                        xml_content = await response.text()
                        
                        try:
                            root = ET.fromstring(xml_content)
                        except:
                            # –ü–æ–ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–æ–π –ø–∞—Ä—Å–µ—Ä –µ—Å–ª–∏ XML –±–∏—Ç—ã–π
                            return articles
                        
                        for item in root.findall('.//item'):
                            title_elem = item.find('title')
                            description_elem = item.find('description')
                            link_elem = item.find('link')
                            pub_date_elem = item.find('pubDate')
                            
                            if title_elem is not None:
                                title = title_elem.text or ''
                                description = description_elem.text if description_elem is not None else ''
                                
                                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                                full_text = f"{title} {description}".lower()
                                
                                # –§–∏–ª—å—Ç—Ä—É–µ–º –¢–û–õ–¨–ö–û —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
                                if self._is_financial_news(full_text):
                                    articles.append({
                                        'id': f"{source_name}_{pub_date_elem.text if pub_date_elem else ''}_{len(articles)}",
                                        'source': source_name,
                                        'title': title,
                                        'description': description,
                                        'content': description,
                                        'url': link_elem.text if link_elem is not None else '',
                                        'published_at': pub_date_elem.text if pub_date_elem else '',
                                        'source_name': source_name,
                                        'fetched_at': datetime.now().isoformat(),
                                        'is_financial': True
                                    })
                        
                        logger.info(f"   ‚úÖ {source_name}: {len(articles)} —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
            
        except asyncio.TimeoutError:
            logger.warning(f"‚è∞ {source_name}: —Ç–∞–π–º–∞—É—Ç")
        except Exception as e:
            logger.debug(f"üîß {source_name} –æ—à–∏–±–∫–∞: {str(e)[:50]}")
        
        return articles
    
    async def fetch_all_news(self) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –í–°–ï–• –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –í–°–ï–• RSS –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
        logger.info("üì• –°–±–æ—Ä —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS...")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ RSS –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        tasks = []
        for source_name, url in self.rss_feeds.items():
            tasks.append(self.fetch_rss_feed(url, source_name))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Ç–∞—Ç—å–∏
        all_articles = []
        for i, result in enumerate(results):
            source_name = list(self.rss_feeds.keys())[i]
            if isinstance(result, list):
                all_articles.extend(result)
                logger.info(f"   üìä {source_name}: {len(result)} —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
            elif isinstance(result, Exception):
                logger.warning(f"   ‚ö†Ô∏è {source_name}: –æ—à–∏–±–∫–∞")
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
        unique_articles = []
        seen_titles = set()
        
        for article in all_articles:
            title_key = article['title'][:80].lower().strip()
            if title_key and title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_articles.append(article)
        
        removed = len(all_articles) - len(unique_articles)
        if removed > 0:
            logger.info(f"üîÑ –£–¥–∞–ª–µ–Ω–æ {removed} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–Ω–æ–≤—ã–µ —Å–Ω–∞—á–∞–ª–∞)
        unique_articles.sort(
            key=lambda x: x.get('published_at', ''),
            reverse=True
        )
        
        logger.info(f"üìä –ò–¢–û–ì–û: {len(unique_articles)} —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        return unique_articles[:25]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 25
    
    def get_source_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º"""
        return {
            'rss_feeds_count': len(self.rss_feeds),
            'financial_keywords': len(self.financial_keywords),
            'mediastack_configured': bool(self.mediastack_key),
            'zenserp_configured': bool(self.zenserp_key)
        }
