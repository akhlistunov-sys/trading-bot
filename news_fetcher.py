import os
import aiohttp
import asyncio
import logging
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import json

logger = logging.getLogger(__name__)

class NewsFetcher:
    """–°–±–æ—Ä—â–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    
    def __init__(self):
        self.newsapi_key = os.getenv("NewsAPI")
        self.zenserp_key = os.getenv("ZENSEPTAPI")
        
        # –ö—ç—à –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π (—á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–π —Å–µ—Å—Å–∏–∏)
        self.news_cache = {}
        
        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ RSS MOEX
        self.moex_feeds = {
            "all_news": "https://moex.com/export/news.aspx?cat=100",
            "main_news": "https://moex.com/export/news.aspx?cat=101"
        }
        
        logger.info("üì∞ NewsFetcher –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    async def fetch_newsapi(self) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ NewsAPI.org"""
        if not self.newsapi_key:
            logger.warning("‚ö†Ô∏è NewsAPI –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
        
        url = "https://newsapi.org/v2/everything"
        
        params = {
            'q': '–∞–∫—Ü–∏–∏ OR –¥–∏–≤–∏–¥–µ–Ω–¥—ã OR –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å OR –∫–≤–∞—Ä—Ç–∞–ª OR –ø—Ä–∏–±—ã–ª—å',
            'language': 'ru',
            'sortBy': 'publishedAt',
            'pageSize': 20,
            'apiKey': self.newsapi_key,
            'from': (datetime.now() - timedelta(hours=24)).strftime('%Y-%m-%dT%H:%M:%S')
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, timeout=10) as response:
                    if response.status == 200:
                        data = await response.json()
                        articles = []
                        
                        for article in data.get('articles', []):
                            articles.append({
                                'id': f"newsapi_{article.get('publishedAt', '')}_{len(articles)}",
                                'source': 'NewsAPI',
                                'title': article.get('title', ''),
                                'description': article.get('description', ''),
                                'content': article.get('content', ''),
                                'url': article.get('url', ''),
                                'published_at': article.get('publishedAt', ''),
                                'author': article.get('author', ''),
                                'source_name': article.get('source', {}).get('name', ''),
                                'fetched_at': datetime.now().isoformat()
                            })
                        
                        logger.info(f"‚úÖ NewsAPI: –ø–æ–ª—É—á–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π")
                        return articles
                    else:
                        logger.error(f"‚ùå NewsAPI –æ—à–∏–±–∫–∞: {response.status}")
                        return []
                        
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ NewsAPI: {e}")
            return []
    
    async def fetch_zenserp(self) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Zenserp (Google News)"""
        if not self.zenserp_key:
            logger.warning("‚ö†Ô∏è Zenserp –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
        
        url = "https://app.zenserp.com/api/v2/search"
        headers = {'apikey': self.zenserp_key}
        
        params = {
            'q': '–∞–∫—Ü–∏–∏ –†–æ—Å—Å–∏—è –±–∏—Ä–∂–∞ –ú–æ—Å–±–∏—Ä–∂–∞ –¥–∏–≤–∏–¥–µ–Ω–¥—ã',
            'tbm': 'nws',
            'num': 15,
            'hl': 'ru',
            'gl': 'ru',
            'tbs': 'qdr:d'
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, params=params, timeout=10) as response:
                    if response.status == 200:
                        data = await response.json()
                        articles = []
                        
                        if 'news_results' in data:
                            for item in data['news_results']:
                                articles.append({
                                    'id': f"zenserp_{item.get('date', '')}_{len(articles)}",
                                    'source': 'Zenserp',
                                    'title': item.get('title', ''),
                                    'description': item.get('snippet', ''),
                                    'url': item.get('url', ''),
                                    'published_at': item.get('date', ''),
                                    'source_name': item.get('source', 'Unknown'),
                                    'fetched_at': datetime.now().isoformat()
                                })
                        
                        logger.info(f"‚úÖ Zenserp: –ø–æ–ª—É—á–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π")
                        return articles
                    else:
                        logger.error(f"‚ùå Zenserp –æ—à–∏–±–∫–∞: {response.status}")
                        return []
                        
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Zenserp: {e}")
            return []
    
    async def fetch_moex_rss(self) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —Å MOEX RSS —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º XML –ø–∞—Ä—Å–µ—Ä–∞"""
        articles = []
        
        try:
            for feed_name, feed_url in self.moex_feeds.items():
                async with aiohttp.ClientSession() as session:
                    async with session.get(feed_url, timeout=10) as response:
                        if response.status == 200:
                            xml_content = await response.text()
                            
                            # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ RSS XML
                            root = ET.fromstring(xml_content)
                            
                            # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã item
                            for item in root.findall('.//item'):
                                title_elem = item.find('title')
                                description_elem = item.find('description')
                                link_elem = item.find('link')
                                pub_date_elem = item.find('pubDate')
                                
                                if title_elem is not None:
                                    articles.append({
                                        'id': f"moex_{pub_date_elem.text if pub_date_elem else ''}_{len(articles)}",
                                        'source': 'MOEX',
                                        'feed_type': feed_name,
                                        'title': title_elem.text or '',
                                        'description': description_elem.text if description_elem is not None else '',
                                        'url': link_elem.text if link_elem is not None else '',
                                        'published_at': pub_date_elem.text if pub_date_elem is not None else '',
                                        'source_name': '–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –±–∏—Ä–∂–∞',
                                        'fetched_at': datetime.now().isoformat()
                                    })
            
            logger.info(f"‚úÖ MOEX RSS: –ø–æ–ª—É—á–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π")
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ MOEX RSS: {e}")
            return []
    
    def _deduplicate_news(self, all_articles: List[Dict]) -> List[Dict]:
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É –∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é"""
        seen_keys = set()
        unique_articles = []
        
        for article in all_articles:
            # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            title_key = article.get('title', '').strip().lower()[:100]
            desc_key = article.get('description', '').strip().lower()[:50]
            cache_key = f"{title_key}_{desc_key}"
            
            if cache_key not in seen_keys:
                seen_keys.add(cache_key)
                unique_articles.append(article)
        
        logger.info(f"üîÑ –£–¥–∞–ª–µ–Ω–æ {len(all_articles) - len(unique_articles)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        return unique_articles
    
    async def fetch_rbc_rss(self) -> List[Dict]:
    """–†–µ–∑–µ—Ä–≤–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫: RSS –ª–µ–Ω—Ç–∞ –†–ë–ö (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)"""
    articles = []
    rss_urls = [
        "https://rssexport.rbc.ru/rbcnews/news/30/full.rss",
        "https://rssexport.rbc.ru/rbcnews/news/20/full.rss"  # –§–∏–Ω–∞–Ω—Å—ã
    ]
    
    try:
        for url in rss_urls:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=10) as response:
                    if response.status == 200:
                        xml_content = await response.text()
                        root = ET.fromstring(xml_content)
                        
                        for item in root.findall('.//item'):
                            title_elem = item.find('title')
                            description_elem = item.find('description')
                            link_elem = item.find('link')
                            pub_date_elem = item.find('pubDate')
                            
                            if title_elem is not None:
                                articles.append({
                                    'id': f"rbc_{pub_date_elem.text if pub_date_elem else ''}_{len(articles)}",
                                    'source': 'RBC',
                                    'title': title_elem.text or '',
                                    'description': description_elem.text if description_elem is not None else '',
                                    'url': link_elem.text if link_elem is not None else '',
                                    'published_at': pub_date_elem.text if pub_date_elem is not None else '',
                                    'source_name': '–†–ë–ö',
                                    'fetched_at': datetime.now().isoformat()
                                })
        
        logger.info(f"‚úÖ RBC RSS: –ø–æ–ª—É—á–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        return articles
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ RBC RSS: {e}")
        return []
        
    async def fetch_all_news(self) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        logger.info("üì• –ù–∞—á–∏–Ω–∞—é —Å–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        newsapi_task = self.fetch_newsapi()
        zenserp_task = self.fetch_zenserp()
        moex_task = self.fetch_moex_rss()
        
        results = await asyncio.gather(
            newsapi_task,
            zenserp_task,
            moex_task,
            rbc_task,
            return_exceptions=True
        )
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        all_articles = []
        for result in results:
            if isinstance(result, list):
                all_articles.extend(result)
            elif isinstance(result, Exception):
                logger.error(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ —Å–±–æ—Ä–µ –Ω–æ–≤–æ—Å—Ç–µ–π: {result}")
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        unique_articles = self._deduplicate_news(all_articles)
        
        logger.info(f"üìä –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {len(unique_articles)}")
        return unique_articles
    
    def get_source_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º"""
        return {
            'newsapi_configured': bool(self.newsapi_key),
            'zenserp_configured': bool(self.zenserp_key),
            'moex_feeds': len(self.moex_feeds),
            'cache_size': len(self.news_cache)
        }
