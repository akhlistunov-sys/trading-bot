import logging
import json
import os
import httpx
import asyncio
from datetime import datetime
from typing import Dict, List, Optional

logger = logging.getLogger(__name__)

class NlpEngine:
    def __init__(self):
        self.providers = {
            'gigachat': {
                'url': 'https://gigachat.devices.sberbank.ru/api/v1/chat/completions',
                'token': os.getenv('GIGACHATAPI'),
                'models': ['GigaChat', 'GigaChat-Pro'],
                'headers': {
                    'Authorization': f'Bearer {os.getenv("GIGACHATAPI")}',
                    'Content-Type': 'application/json'
                }
            },
            'openrouter': {
                'url': 'https://openrouter.ai/api/v1/chat/completions',
                'token': os.getenv('OPENROUTER_API_TOKEN'),
                'models': [
                    'google/gemini-2.0-flash-exp:free',
                    'mistralai/mistral-7b-instruct:free'
                ],
                'headers': {
                    'Authorization': f'Bearer {os.getenv("OPENROUTER_API_TOKEN")}',
                    'Content-Type': 'application/json'
                }
            }
        }
        
        self.current_model_idx = 0
        self.model = self.model_priority[self.current_model_idx]
        self.api_url = "https://openrouter.ai/api/v1/chat/completions"
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.total_requests = 0
        self.successful_requests = 0
        self.model_switches = 0
        self.analysis_cache = {}
        self.cache_hits = 0
        self.cache_misses = 0
        
        # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        self.request_delay = 2
        
        logger.info(f"ü§ñ NLP-–¥–≤–∏–∂–æ–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        logger.info(f"üß† –ú–æ–¥–µ–ª–∏ –≤ –∫–∞—Å–∫–∞–¥–µ ({len(self.model_priority)}):")
        for i, model in enumerate(self.model_priority):
            status = "‚úÖ" if i == 0 else "üß™" if "deepseek" in model else "üîß"
            logger.info(f"   {i+1}. {status} {model}")
    
    def _switch_to_next_model(self):
        """–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å –≤ –∫–∞—Å–∫–∞–¥–µ"""
        old_model = self.model
        self.current_model_idx = (self.current_model_idx + 1) % len(self.model_priority)
        self.model = self.model_priority[self.current_model_idx]
        self.model_switches += 1
        
        logger.info(f"üîÑ –°–º–µ–Ω–∞ –º–æ–¥–µ–ª–∏: {old_model} ‚Üí {self.model}")
        return self.model
    
    def _create_cache_key(self, news_item: Dict) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª—é—á–∞ –¥–ª—è –∫—ç—à–∞"""
        title = news_item.get('title', '')[:50].replace(' ', '_').lower()
        source = news_item.get('source', '')[:20].replace(' ', '_').lower()
        content_hash = hash(news_item.get('content', '')[:100]) % 10000
        return f"{source}_{title}_{content_hash}"
    
    def _create_analysis_prompt(self, news_item: Dict) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–∏"""
        
        title = news_item.get('title', '')
        description = news_item.get('description', '')
        content = news_item.get('content', '') or description
        source = news_item.get('source_name', news_item.get('source', 'Unknown'))
        
        prompt = f"""
        ===== –ê–ù–ê–õ–ò–ó –§–ò–ù–ê–ù–°–û–í–û–ô –ù–û–í–û–°–¢–ò =====
        
        üì∞ –ò–°–¢–û–ß–ù–ò–ö: {source}
        üè∑Ô∏è –ó–ê–ì–û–õ–û–í–û–ö: {title}
        
        üìù –¢–ï–ö–°–¢ –ù–û–í–û–°–¢–ò:
        {content[:1200]}
        
        ===== –ò–ù–°–¢–†–£–ö–¶–ò–Ø –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê =====
        
        –¢—ã ‚Äî —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –ò–ò. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –Ω–æ–≤–æ—Å—Ç—å –≤—ã—à–µ –∏ –æ—Ç–≤–µ—Ç—å –í –°–¢–†–û–ì–û–ú JSON –§–û–†–ú–ê–¢–ï.
        
        –í–û–ó–í–†–ê–©–ê–ô –¢–û–õ–¨–ö–û JSON –í –°–õ–ï–î–£–Æ–©–ï–ú –§–û–†–ú–ê–¢–ï:
        {{
            "analysis": {{
                "tickers": ["TICKER1", "TICKER2"],
                "event_type": "earnings_report | dividend | merger_acquisition | regulatory | geopolitical | market_update | corporate_action | other",
                "impact_score": 1-10,
                "relevance_score": 1-100,
                "sentiment": "positive | negative | neutral | mixed",
                "horizon": "immediate | short_term | medium_term | long_term",
                "summary": "–∫—Ä–∞—Ç–∫–∞—è —Å—É—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)"
            }}
        }}
        
        –ò–ù–°–¢–†–£–ö–¶–ò–ò:
        1. –ò–∑–≤–ª–µ–∫–∏ –≤—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π –∏ –∏—Ö —Ç–∏–∫–µ—Ä–æ–≤ (–ø—Ä–∏–º–µ—Ä—ã: "–°–±–µ—Ä–±–∞–Ω–∫" ‚Üí SBER, "–ì–∞–∑–ø—Ä–æ–º" ‚Üí GAZP, "–õ—É–∫–æ–π–ª" ‚Üí LKOH)
        2. –û—Ü–µ–Ω–∏ –≤–∞–∂–Ω–æ—Å—Ç—å (impact_score): 1-3=–Ω–∏–∑–∫–∞—è, 4-6=—Å—Ä–µ–¥–Ω—è—è, 7-8=–≤—ã—Å–æ–∫–∞—è, 9-10=–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è
        3. –û—Ü–µ–Ω–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Ç—Ä–µ–π–¥–∏–Ω–≥–∞ (relevance_score): 1-100
        4. –û–ø—Ä–µ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –∞–∫—Ü–∏–π
        5. –ö—Ä–∞—Ç–∫–æ –æ–±—ä—è—Å–Ω–∏ —Å—É—Ç—å
        
        –¢–û–õ–¨–ö–û JSON, –ë–ï–ó –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û–ì–û –¢–ï–ö–°–¢–ê, –ë–ï–ó –ú–ê–†–ö–î–ê–£–ù–ê, –ë–ï–ó –û–ë–™–Ø–°–ù–ï–ù–ò–ô!
        """
        
        return prompt
    
    async def analyze_news(self, news_item: Dict) -> Optional[Dict]:
        """–ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é –ò–ò"""
        
        self.total_requests += 1
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        cache_key = self._create_cache_key(news_item)
        if cache_key in self.analysis_cache:
            self.cache_hits += 1
            logger.info(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É—é –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (hits: {self.cache_hits})")
            return self.analysis_cache[cache_key]
        
        self.cache_misses += 1
        news_title = news_item.get('title', '')[:50]
        logger.info(f"üß† –ê–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–∏ #{self.total_requests}: {news_title}...")
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
        max_retries = min(3, len(self.model_priority))
        last_error = None
        
        for attempt in range(max_retries):
            try:
                logger.info(f"üì® –ü–æ–ø—ã—Ç–∫–∞ {attempt+1}/{max_retries} —Å –º–æ–¥–µ–ª—å—é: {self.model}")
                
                prompt = self._create_analysis_prompt(news_item)
                
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.post(
                        url=self.api_url,
                        headers={
                            "Authorization": f"Bearer {self.api_key}",
                            "Content-Type": "application/json",
                            "HTTP-Referer": "https://github.com",
                            "X-Title": "News NLP Trading AI"
                        },
                        json={
                            "model": self.model,
                            "messages": [
                                {
                                    "role": "system",
                                    "content": "–¢—ã ‚Äî —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –ò–ò. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –Ω–æ–≤–æ—Å—Ç–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–π —Å—Ç—Ä–æ–≥–æ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ. –ù–∏–∫–∞–∫–æ–≥–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞!"
                                },
                                {"role": "user", "content": prompt}
                            ],
                            "temperature": 0.1,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
                            "max_tokens": 600
                        }
                    )
                
                logger.info(f"üì• –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: —Å—Ç–∞—Ç—É—Å {response.status_code}")
                
                if response.status_code == 200:
                    result = response.json()
                    ai_response = result["choices"][0]["message"]["content"]
                    
                    self.successful_requests += 1
                    
                    # –ü–∞—Ä—Å–∏–Ω–≥ JSON –æ—Ç–≤–µ—Ç–∞
                    analysis_result = self._parse_ai_response(ai_response, news_item)
                    
                    if analysis_result:
                        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                        self.analysis_cache[cache_key] = analysis_result
                        if len(self.analysis_cache) > 50:
                            oldest = next(iter(self.analysis_cache))
                            del self.analysis_cache[oldest]
                        
                        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–∏ (–º–æ–¥–µ–ª—å: {self.model})")
                        return analysis_result
                    else:
                        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –æ—Ç–≤–µ—Ç –ò–ò")
                        last_error = "Parse error"
                        
                elif response.status_code in [400, 404]:
                    # –ü—Ä–æ–±–ª–µ–º–∞ —Å –º–æ–¥–µ–ª—å—é
                    error_data = response.json()
                    error_msg = error_data.get('error', {}).get('message', 'Unknown error')
                    
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏ {self.model}: {error_msg[:100]}")
                    
                    if attempt < max_retries - 1:
                        self._switch_to_next_model()
                        logger.info(f"‚è≥ –ó–∞–¥–µ—Ä–∂–∫–∞ {self.request_delay} —Å–µ–∫ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –º–æ–¥–µ–ª—å—é...")
                        await asyncio.sleep(self.request_delay)
                        continue
                    else:
                        last_error = f"–í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã: {error_msg}"
                        break
                
                elif response.status_code == 429:
                    # Rate limit
                    logger.warning(f"‚ö†Ô∏è Rate limit –¥–ª—è –º–æ–¥–µ–ª–∏ {self.model}")
                    
                    if attempt < max_retries - 1:
                        self._switch_to_next_model()
                        logger.info(f"‚è≥ –ó–∞–¥–µ—Ä–∂–∫–∞ {self.request_delay * 2} —Å–µ–∫ –∏–∑-–∑–∞ rate limit...")
                        await asyncio.sleep(self.request_delay * 2)
                        continue
                    else:
                        last_error = "Rate limit –Ω–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª—è—Ö"
                        break
                
                else:
                    last_error = f"HTTP {response.status_code}"
                    break
                    
            except httpx.TimeoutException:
                last_error = "–¢–∞–π–º–∞—É—Ç 30—Å"
                logger.error(f"‚è∞ –¢–∞–π–º–∞—É—Ç –Ω–∞ –º–æ–¥–µ–ª–∏ {self.model}")
                if attempt < max_retries - 1:
                    self._switch_to_next_model()
                    await asyncio.sleep(3)
                    continue
                break
                
            except Exception as e:
                last_error = str(e)
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)[:100]}")
                break
        
        if last_error:
            logger.error(f"‚ùå –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∞–Ω–∞–ª–∏–∑–∞ failed: {last_error}")
        
        return None
    
    def _parse_ai_response(self, response: str, news_item: Dict) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ –ò–ò"""
        try:
            # –û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ - —É–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–π —Ç–µ–∫—Å—Ç
            response = response.strip()
            
            # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ (—É–¥–∞–ª—è–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ markdown –∫–æ–¥–æ–≤—ã–µ –±–ª–æ–∫–∏)
            if '```json' in response:
                response = response.split('```json')[1].split('```')[0].strip()
            elif '```' in response:
                response = response.split('```')[1].split('```')[0].strip()
            
            # –ò—â–µ–º –ø–µ—Ä–≤—ã–π { –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π }
            start_idx = response.find('{')
            end_idx = response.rfind('}') + 1
            
            if start_idx == -1 or end_idx == 0:
                logger.warning(f"‚ö†Ô∏è –ò–ò –Ω–µ –≤–µ—Ä–Ω—É–ª JSON: {response[:100]}...")
                return None
            
            json_str = response[start_idx:end_idx]
            data = json.loads(json_str)
            
            analysis_data = data.get("analysis", {})
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
            required_fields = ['tickers', 'event_type', 'impact_score', 'relevance_score']
            if not all(field in analysis_data for field in required_fields):
                logger.warning("‚ö†Ô∏è –í –æ—Ç–≤–µ—Ç–µ –ò–ò –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è")
                return None
            
            # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
            result = {
                'news_id': news_item.get('id', ''),
                'news_title': news_item.get('title', ''),
                'news_source': news_item.get('source', ''),
                'news_url': news_item.get('url', ''),
                'analysis_timestamp': datetime.now().isoformat(),
                
                # –ê–Ω–∞–ª–∏–∑ –ò–ò
                'tickers': analysis_data.get('tickers', []),
                'event_type': analysis_data.get('event_type', 'other'),
                'impact_score': int(analysis_data.get('impact_score', 1)),
                'relevance_score': int(analysis_data.get('relevance_score', 30)),
                'sentiment': analysis_data.get('sentiment', 'neutral'),
                'horizon': analysis_data.get('horizon', 'short_term'),
                'summary': analysis_data.get('summary', ''),
                
                # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                'ai_model': self.model,
                'confidence': min(1.0, analysis_data.get('relevance_score', 30) / 100.0)
            }
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            tickers_str = ', '.join(result['tickers']) if result['tickers'] else '–ù–ï–¢ –¢–ò–ö–ï–†–û–í'
            logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç: {tickers_str} | Impact: {result['impact_score']}/10 | Relevance: {result['relevance_score']}/100")
            
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON –æ—Ç –ò–ò: {str(e)[:50]}")
            logger.debug(f"üí¨ –û—Ç–≤–µ—Ç –ò–ò: {response[:200]}")
            return None
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞ –ò–ò: {str(e)[:50]}")
            return None
    
    def get_current_model(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –∞–∫—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        return self.model
    
    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–±–æ—Ç—ã NLP-–¥–≤–∏–∂–∫–∞"""
        success_rate = (self.successful_requests / self.total_requests * 100) if self.total_requests > 0 else 0
        
        return {
            'total_requests': self.total_requests,
            'successful_requests': self.successful_requests,
            'success_rate': round(success_rate, 1),
            'current_model': self.model,
            'model_index': self.current_model_idx,
            'model_switches': self.model_switches,
            'cache_size': len(self.analysis_cache),
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'cache_hit_rate': round((self.cache_hits / (self.cache_hits + self.cache_misses) * 100), 1) if (self.cache_hits + self.cache_misses) > 0 else 0
        }
