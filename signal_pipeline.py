# signal_pipeline.py - ÐŸÐžÐ¡Ð›Ð•Ð”ÐžÐ’ÐÐ¢Ð•Ð›Ð¬ÐÐÐ¯ ÐžÐ‘Ð ÐÐ‘ÐžÐ¢ÐšÐ
import logging
import asyncio
from datetime import datetime
from typing import Dict, List, Optional
import json

logger = logging.getLogger(__name__)

class SignalPipeline:
    """ÐšÐ¾Ð½Ð²ÐµÐ¹ÐµÑ€ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð½Ð¾Ð²Ð¾ÑÑ‚ÐµÐ¹ Ñ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¾Ð¹ GigaChat"""
    
    def __init__(self, nlp_engine, finam_verifier, risk_manager, enhanced_analyzer, news_prefilter):
        self.nlp_engine = nlp_engine
        self.finam_verifier = finam_verifier
        self.risk_manager = risk_manager
        self.enhanced_analyzer = enhanced_analyzer
        self.news_prefilter = news_prefilter
        
        self.stats = {
            'total_news': 0,
            'filtered_news': 0,
            'analyzed_news': 0,
            'verified_signals': 0,
            'executed_signals': 0,
            'pipeline_start': datetime.now().isoformat()
        }
        
        logger.info("ðŸš€ SignalPipeline Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ñ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¾Ð¹")
        logger.info("   Ð­Ñ‚Ð°Ð¿Ñ‹: PreFilter â†’ EnhancedAnalyzer â†’ NLP (Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾)")
    
    async def process_news_batch(self, news_list: List[Dict]) -> List[Dict]:
        """ÐŸÐ°ÐºÐµÑ‚Ð½Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð½Ð¾Ð²Ð¾ÑÑ‚ÐµÐ¹ ÐŸÐžÐ¡Ð›Ð•Ð”ÐžÐ’ÐÐ¢Ð•Ð›Ð¬ÐÐž"""
        
        signals = []
        self.stats['total_news'] += len(news_list)
        
        logger.info(f"ðŸ“Š ÐÐ°Ñ‡Ð°Ð»Ð¾ ÐŸÐžÐ¡Ð›Ð•Ð”ÐžÐ’ÐÐ¢Ð•Ð›Ð¬ÐÐžÐ™ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ {len(news_list)} Ð½Ð¾Ð²Ð¾ÑÑ‚ÐµÐ¹")
        
        # ÐŸÐžÐ¡Ð›Ð•Ð”ÐžÐ’ÐÐ¢Ð•Ð›Ð¬ÐÐÐ¯ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° (Ð½Ðµ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð°Ñ!)
        processed_count = 0
        for news_item in news_list:
            try:
                signal = await self._process_single_news(news_item)
                if signal:
                    signals.append(signal)
                    logger.info(f"   âœ… ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð° Ð½Ð¾Ð²Ð¾ÑÑ‚ÑŒ {processed_count + 1}/{len(news_list)}: Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ {len(signals)} ÑÐ¸Ð³Ð½Ð°Ð»Ð¾Ð²")
                else:
                    logger.debug(f"   â­ï¸ ÐÐ¾Ð²Ð¾ÑÑ‚ÑŒ {processed_count + 1}/{len(news_list)} Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð°")
                
                processed_count += 1
                
                # ÐŸÐ°ÑƒÐ·Ð° Ð¼ÐµÐ¶Ð´Ñƒ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¾Ð¹ Ð½Ð¾Ð²Ð¾ÑÑ‚ÐµÐ¹ Ð´Ð»Ñ GigaChat
                if processed_count % 3 == 0:  # ÐšÐ°Ð¶Ð´Ñ‹Ðµ 3 Ð½Ð¾Ð²Ð¾ÑÑ‚Ð¸
                    await asyncio.sleep(2)
                    
            except Exception as e:
                logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð½Ð¾Ð²Ð¾ÑÑ‚Ð¸ {processed_count + 1}: {str(e)[:100]}")
                continue
        
        self.stats['executed_signals'] += len(signals)
        
        logger.info(f"ðŸ“Š Ð˜Ñ‚Ð¾Ð³Ð¸ ÐŸÐžÐ¡Ð›Ð•Ð”ÐžÐ’ÐÐ¢Ð•Ð›Ð¬ÐÐžÐ™ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸:")
        logger.info(f"   ÐÐ¾Ð²Ð¾ÑÑ‚Ð¸: {self.stats['total_news']}")
        logger.info(f"   ÐžÑ‚Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð²Ð°Ð½Ð¾: {self.stats['filtered_news']}")
        logger.info(f"   ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾: {self.stats['analyzed_news']}")
        logger.info(f"   Ð¡Ð¸Ð³Ð½Ð°Ð»Ð¾Ð²: {len(signals)}")
        
        return signals
    
    async def _process_single_news(self, news_item: Dict) -> Optional[Dict]:
        """ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¾Ð´Ð½Ð¾Ð¹ Ð½Ð¾Ð²Ð¾ÑÑ‚Ð¸ Ñ‡ÐµÑ€ÐµÐ· Ð²ÑÐµ ÑÑ‚Ð°Ð¿Ñ‹"""
        
        # 1. ÐŸÑ€Ðµ-Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ñ
        if not self.news_prefilter.is_tradable(news_item):
            self.stats['filtered_news'] += 1
            logger.debug(f"   âŒ PreFilter Ð¾Ñ‚ÑÐµÑÐ»: {news_item.get('title', '')[:50]}")
            return None
        
        # 2. Ð‘Ñ‹ÑÑ‚Ñ€Ð°Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° EnhancedAnalyzer (ÑƒÐ¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ð°Ñ)
        if not self.enhanced_analyzer.quick_filter(news_item):
            logger.debug(f"   âŒ EnhancedAnalyzer Ð¾Ñ‚ÑÐµÑÐ»: {news_item.get('title', '')[:50]}")
            return None
        
        # 3. NLP Ð°Ð½Ð°Ð»Ð¸Ð· (GigaChat/OpenRouter) - ÐŸÐžÐ¡Ð›Ð•Ð”ÐžÐ’ÐÐ¢Ð•Ð›Ð¬ÐÐž
        logger.debug(f"   ðŸ“¡ ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÑŽ Ð² NLP: {news_item.get('title', '')[:60]}")
        nlp_analysis = await self.nlp_engine.analyze_news(news_item)
        
        # 4. Fallback: EnhancedAnalyzer ÐµÑÐ»Ð¸ Ð˜Ð˜ Ð½Ðµ ÑÑ€Ð°Ð±Ð¾Ñ‚Ð°Ð»
        if not nlp_analysis:
            nlp_analysis = self.enhanced_analyzer.analyze_news(news_item)
            if nlp_analysis:
                nlp_analysis['ai_provider'] = 'enhanced_fallback'
                logger.debug(f"   ðŸ”§ Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽ EnhancedAnalyzer fallback")
        
        if not nlp_analysis:
            logger.debug(f"   âŒ NLP Ð½Ðµ Ð´Ð°Ð» Ð°Ð½Ð°Ð»Ð¸Ð·Ð°")
            return None
        
        self.stats['analyzed_news'] += 1
        logger.debug(f"   âœ… NLP Ð°Ð½Ð°Ð»Ð¸Ð· Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½ Ð¾Ñ‚ {nlp_analysis.get('ai_provider', 'unknown')}")
        
        # 5. Ð’ÐµÑ€Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ Ñ‡ÐµÑ€ÐµÐ· Finam
        verification = await self.finam_verifier.verify_signal(nlp_analysis)
        
        if not verification['valid']:
            logger.debug(f"   âŒ Finam Ð²ÐµÑ€Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð½Ðµ Ð¿Ñ€Ð¾ÑˆÐ»Ð°: {verification.get('reason', '')}")
            return None
        
        logger.debug(f"   âœ… Finam Ð²ÐµÑ€Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð¿Ñ€Ð¾Ð¹Ð´ÐµÐ½Ð°")
        
        # 6. ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ñ… Ñ†ÐµÐ½
        tickers = verification.get('tickers', [])
        if not tickers:
            return None
        
        current_prices = await self.finam_verifier.get_current_prices(tickers)
        
        # 7. Risk Manager Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° ÑÐ¸Ð³Ð½Ð°Ð»Ð°
        signal = self.risk_manager.prepare_signal(
            analysis=nlp_analysis,
            verification=verification,
            current_prices=current_prices
        )
        
        if signal:
            self.stats['verified_signals'] += 1
            
            # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
            signal.update({
                'pipeline_version': '2.1',
                'processing_timestamp': datetime.now().isoformat(),
                'verification_details': verification.get('details', {}),
                'nlp_analysis': {
                    'provider': nlp_analysis.get('ai_provider'),
                    'event_type': nlp_analysis.get('event_type'),
                    'sentiment': nlp_analysis.get('sentiment'),
                    'confidence': nlp_analysis.get('confidence')
                }
            })
            
            logger.info(f"âœ… Ð¡Ð˜Ð“ÐÐÐ› ÑÑ„Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½: {signal['action']} {signal['ticker']}")
        
        return signal
    
    def get_stats(self) -> Dict:
        """ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸ ÐºÐ¾Ð½Ð²ÐµÐ¹ÐµÑ€Ð°"""
        total_processed = self.stats['total_news']
        
        if total_processed > 0:
            filter_rate = (self.stats['filtered_news'] / total_processed) * 100
            analysis_rate = (self.stats['analyzed_news'] / total_processed) * 100
            signal_rate = (self.stats['verified_signals'] / total_processed) * 100
        else:
            filter_rate = analysis_rate = signal_rate = 0
        
        return {
            **self.stats,
            'filter_rate_percent': round(filter_rate, 1),
            'analysis_rate_percent': round(analysis_rate, 1),
            'signal_rate_percent': round(signal_rate, 1),
            'efficiency': round((self.stats['verified_signals'] / max(1, self.stats['analyzed_news'])) * 100, 1),
            'current_time': datetime.now().isoformat(),
            'processing_mode': 'sequential_gigachat'
        }
