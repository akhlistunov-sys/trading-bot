# signal_pipeline.py
import logging
import asyncio
from datetime import datetime
from typing import Dict, List, Optional
import json

logger = logging.getLogger(__name__)

class SignalPipeline:
    """–ö–æ–Ω–≤–µ–π–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ —Ç–æ—Ä–≥–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã"""
    
    def __init__(self, nlp_engine, finam_verifier, risk_manager, enhanced_analyzer, news_prefilter):
        self.nlp_engine = nlp_engine
        self.finam_verifier = finam_verifier
        self.risk_manager = risk_manager
        self.enhanced_analyzer = enhanced_analyzer
        self.news_prefilter = news_prefilter
        
        self.stats = {
            'total_news': 0,
            'filtered_news': 0,
            'analyzed_news': 0,
            'verified_signals': 0,
            'executed_signals': 0,
            'pipeline_start': datetime.now().isoformat()
        }
        
        logger.info("üöÄ SignalPipeline –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        logger.info("   –≠—Ç–∞–ø—ã: PreFilter ‚Üí NLP ‚Üí Finam ‚Üí RiskManager")
    
    async def process_news_batch(self, news_list: List[Dict]) -> List[Dict]:
        """–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"""
        
        signals = []
        self.stats['total_news'] += len(news_list)
        
        logger.info(f"üìä –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {len(news_list)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–Ω–æ —Å –ª–∏–º–∏—Ç–æ–º)
        batch_size = min(10, len(news_list))
        
        for i in range(0, len(news_list), batch_size):
            batch = news_list[i:i+batch_size]
            
            batch_signals = await self._process_batch(batch)
            signals.extend(batch_signals)
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
            if i + batch_size < len(news_list):
                await asyncio.sleep(1)
        
        self.stats['executed_signals'] += len(signals)
        
        logger.info(f"üìä –ò—Ç–æ–≥–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
        logger.info(f"   –ù–æ–≤–æ—Å—Ç–∏: {self.stats['total_news']}")
        logger.info(f"   –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {self.stats['filtered_news']}")
        logger.info(f"   –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {self.stats['analyzed_news']}")
        logger.info(f"   –°–∏–≥–Ω–∞–ª–æ–≤: {len(signals)}")
        
        return signals
    
    async def _process_batch(self, news_batch: List[Dict]) -> List[Dict]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"""
        batch_signals = []
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏
        tasks = []
        for news_item in news_batch:
            task = self._process_single_news(news_item)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏: {result}")
                continue
            
            if result:
                batch_signals.append(result)
        
        return batch_signals
    
    async def _process_single_news(self, news_item: Dict) -> Optional[Dict]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –≤—Å–µ —ç—Ç–∞–ø—ã"""
        
        # 1. –ü—Ä–µ-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        if not self.news_prefilter.is_tradable(news_item):
            self.stats['filtered_news'] += 1
            return None
        
        # 2. –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ EnhancedAnalyzer
        if not self.enhanced_analyzer.quick_filter(news_item):
            return None
        
        # 3. NLP –∞–Ω–∞–ª–∏–∑ (GigaChat/OpenRouter)
        nlp_analysis = await self.nlp_engine.analyze_news(news_item)
        
        # 4. Fallback: EnhancedAnalyzer –µ—Å–ª–∏ –ò–ò –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª
        if not nlp_analysis:
            nlp_analysis = self.enhanced_analyzer.analyze_news(news_item)
            if nlp_analysis:
                nlp_analysis['ai_provider'] = 'enhanced_fallback'
        
        if not nlp_analysis:
            return None
        
        self.stats['analyzed_news'] += 1
        
        # 5. –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ Finam
        verification = await self.finam_verifier.verify_signal(nlp_analysis)
        
        if not verification['valid']:
            return None
        
        # 6. –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö —Ü–µ–Ω
        tickers = verification.get('tickers', [])
        if not tickers:
            return None
        
        current_prices = await self.finam_verifier.get_current_prices(tickers)
        
        # 7. Risk Manager –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–∏–≥–Ω–∞–ª–∞
        signal = self.risk_manager.prepare_signal(
            analysis=nlp_analysis,
            verification=verification,
            current_prices=current_prices
        )
        
        if signal:
            self.stats['verified_signals'] += 1
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            signal.update({
                'pipeline_version': '2.0',
                'processing_timestamp': datetime.now().isoformat(),
                'verification_details': verification.get('details', {}),
                'nlp_analysis': {
                    'provider': nlp_analysis.get('ai_provider'),
                    'event_type': nlp_analysis.get('event_type'),
                    'sentiment': nlp_analysis.get('sentiment'),
                    'confidence': nlp_analysis.get('confidence')
                }
            })
            
            logger.info(f"‚úÖ –°–ò–ì–ù–ê–õ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω: {signal['action']} {signal['ticker']}")
        
        return signal
    
    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ–Ω–≤–µ–π–µ—Ä–∞"""
        total_processed = self.stats['total_news']
        
        if total_processed > 0:
            filter_rate = (self.stats['filtered_news'] / total_processed) * 100
            analysis_rate = (self.stats['analyzed_news'] / total_processed) * 100
            signal_rate = (self.stats['verified_signals'] / total_processed) * 100
        else:
            filter_rate = analysis_rate = signal_rate = 0
        
        return {
            **self.stats,
            'filter_rate_percent': round(filter_rate, 1),
            'analysis_rate_percent': round(analysis_rate, 1),
            'signal_rate_percent': round(signal_rate, 1),
            'efficiency': round((self.stats['verified_signals'] / max(1, self.stats['analyzed_news'])) * 100, 1),
            'current_time': datetime.now().isoformat()
        }
