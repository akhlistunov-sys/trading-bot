import os
import aiohttp
import asyncio
import logging
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import json

logger = logging.getLogger(__name__)

class NewsFetcher:
    """–°–±–æ—Ä—â–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å —Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    def __init__(self):
        # –ö–ª—é—á–∏ API - Mediastack —Ç–µ–ø–µ—Ä—å –æ—Å–Ω–æ–≤–Ω–æ–π, Zenserp —Ä–µ–∑–µ—Ä–≤–Ω—ã–π
        self.mediastack_key = os.getenv("mediastackAPI")
        self.zenserp_key = os.getenv("ZENSEPTAPI")
        
        # –ö—ç—à –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π
        self.news_cache = {}
        
        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ RSS
        self.moex_feeds = {
            "all_news": "https://moex.com/export/news.aspx?cat=100",
            "main_news": "https://moex.com/export/news.aspx?cat=101"
        }
        
        # –†–µ–∑–µ—Ä–≤–Ω—ã–π RSS –†–ë–ö
        self.rbc_feeds = [
            "https://rssexport.rbc.ru/rbcnews/news/30/full.rss",  # –ì–ª–∞–≤–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
            "https://rssexport.rbc.ru/rbcnews/news/20/full.rss"   # –§–∏–Ω–∞–Ω—Å—ã
        ]
        
        logger.info("üì∞ NewsFetcher –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        logger.info(f"üîë –ò—Å—Ç–æ—á–Ω–∏–∫–∏: Mediastack={'‚úÖ' if self.mediastack_key else '‚ùå'}, "
                   f"Zenserp={'‚úÖ' if self.zenserp_key else '‚ùå'}, "
                   f"MOEX RSS=‚úÖ, RBC RSS=‚úÖ")
    
    async def fetch_mediastack(self) -> List[Dict]:
        """–û–°–ù–û–í–ù–û–ô –ò–°–¢–û–ß–ù–ò–ö: –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Mediastack API"""
        if not self.mediastack_key:
            logger.warning("‚ö†Ô∏è Mediastack –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
        
        url = "http://api.mediastack.com/v1/news"
        
        params = {
            'access_key': self.mediastack_key,
            'languages': 'ru',
            'keywords': '–∞–∫—Ü–∏–∏ –¥–∏–≤–∏–¥–µ–Ω–¥—ã –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–±—ã–ª—å –±–∏—Ä–∂–∞ –ú–æ—Å–±–∏—Ä–∂–∞',
            'limit': 25,
            'sort': 'published_desc',
            'countries': 'ru',
            'categories': 'business',
            'date': (datetime.now() - timedelta(hours=24)).strftime('%Y-%m-%d')
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, timeout=15) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—à–∏–±–∫–∏ API
                        if 'error' in data:
                            error_msg = data['error'].get('message', 'Unknown error')
                            logger.error(f"‚ùå Mediastack API error: {error_msg}")
                            return []
                        
                        articles = []
                        
                        for article in data.get('data', []):
                            articles.append({
                                'id': f"mediastack_{article.get('published_at', '')}_{len(articles)}",
                                'source': 'Mediastack',
                                'title': article.get('title', ''),
                                'description': article.get('description', ''),
                                'content': article.get('description', ''),  # –ò—Å–ø–æ–ª—å–∑—É–µ–º description –∫–∞–∫ content
                                'url': article.get('url', ''),
                                'published_at': article.get('published_at', ''),
                                'author': article.get('author', ''),
                                'source_name': article.get('source', ''),
                                'category': article.get('category', 'business'),
                                'fetched_at': datetime.now().isoformat()
                            })
                        
                        logger.info(f"‚úÖ Mediastack: –ø–æ–ª—É—á–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π")
                        return articles
                    else:
                        logger.error(f"‚ùå Mediastack HTTP –æ—à–∏–±–∫–∞: {response.status}")
                        return []
                        
        except asyncio.TimeoutError:
            logger.error("‚ùå Mediastack: —Ç–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞")
            return []
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Mediastack: {str(e)[:100]}")
            return []
    
    async def fetch_zenserp(self) -> List[Dict]:
        """–†–ï–ó–ï–†–í–ù–´–ô –ò–°–¢–û–ß–ù–ò–ö: –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Zenserp (Google News)"""
        if not self.zenserp_key:
            logger.warning("‚ö†Ô∏è Zenserp –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
        
        url = "https://app.zenserp.com/api/v2/search"
        headers = {'apikey': self.zenserp_key}
        
        params = {
            'q': '–∞–∫—Ü–∏–∏ –†–æ—Å—Å–∏—è –±–∏—Ä–∂–∞ –ú–æ—Å–±–∏—Ä–∂–∞ –¥–∏–≤–∏–¥–µ–Ω–¥—ã –∫–≤–∞—Ä—Ç–∞–ª –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å',
            'tbm': 'nws',
            'num': 15,
            'hl': 'ru',
            'gl': 'ru',
            'tbs': 'qdr:d'  # –ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, params=params, timeout=15) as response:
                    if response.status == 200:
                        data = await response.json()
                        articles = []
                        
                        if 'news_results' in data:
                            for item in data['news_results']:
                                articles.append({
                                    'id': f"zenserp_{item.get('date', '')}_{len(articles)}",
                                    'source': 'Zenserp',
                                    'title': item.get('title', ''),
                                    'description': item.get('snippet', ''),
                                    'url': item.get('url', ''),
                                    'published_at': item.get('date', ''),
                                    'source_name': item.get('source', 'Unknown'),
                                    'fetched_at': datetime.now().isoformat()
                                })
                        
                        logger.info(f"‚úÖ Zenserp: –ø–æ–ª—É—á–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π")
                        return articles
                    else:
                        logger.error(f"‚ùå Zenserp –æ—à–∏–±–∫–∞: {response.status}")
                        return []
                        
        except asyncio.TimeoutError:
            logger.error("‚ùå Zenserp: —Ç–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞")
            return []
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Zenserp: {e}")
            return []
    
    async def fetch_moex_rss(self) -> List[Dict]:
        """–ò–°–¢–û–ß–ù–ò–ö –° –ë–ò–†–ñ–ò: –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —Å MOEX RSS"""
        articles = []
        
        try:
            for feed_name, feed_url in self.moex_feeds.items():
                async with aiohttp.ClientSession() as session:
                    async with session.get(feed_url, timeout=10) as response:
                        if response.status == 200:
                            xml_content = await response.text()
                            
                            # –ü–∞—Ä—Å–∏–Ω–≥ RSS XML
                            root = ET.fromstring(xml_content)
                            
                            for item in root.findall('.//item'):
                                title_elem = item.find('title')
                                description_elem = item.find('description')
                                link_elem = item.find('link')
                                pub_date_elem = item.find('pubDate')
                                
                                if title_elem is not None:
                                    articles.append({
                                        'id': f"moex_{pub_date_elem.text if pub_date_elem else ''}_{len(articles)}",
                                        'source': 'MOEX',
                                        'feed_type': feed_name,
                                        'title': title_elem.text or '',
                                        'description': description_elem.text if description_elem is not None else '',
                                        'content': description_elem.text if description_elem is not None else '',
                                        'url': link_elem.text if link_elem is not None else '',
                                        'published_at': pub_date_elem.text if pub_date_elem is not None else '',
                                        'source_name': '–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –±–∏—Ä–∂–∞',
                                        'fetched_at': datetime.now().isoformat()
                                    })
            
            logger.info(f"‚úÖ MOEX RSS: –ø–æ–ª—É—á–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π")
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ MOEX RSS: {e}")
            return []
    
    async def fetch_rbc_rss(self) -> List[Dict]:
        """–≠–ö–°–¢–†–ï–ù–ù–´–ô –ò–°–¢–û–ß–ù–ò–ö: RSS –ª–µ–Ω—Ç–∞ –†–ë–ö (–≤—Å–µ–≥–¥–∞ –±–µ—Å–ø–ª–∞—Ç–Ω–æ)"""
        articles = []
        
        try:
            for url in self.rbc_feeds:
                async with aiohttp.ClientSession() as session:
                    async with session.get(url, timeout=10) as response:
                        if response.status == 200:
                            xml_content = await response.text()
                            root = ET.fromstring(xml_content)
                            
                            for item in root.findall('.//item'):
                                title_elem = item.find('title')
                                description_elem = item.find('description')
                                link_elem = item.find('link')
                                pub_date_elem = item.find('pubDate')
                                
                                if title_elem is not None:
                                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
                                    title = title_elem.text or ''
                                    description = description_elem.text if description_elem is not None else ''
                                    text = (title + ' ' + description).lower()
                                    
                                    financial_keywords = ['–∞–∫—Ü–∏', '–±–∏—Ä–∂', '–¥–∏–≤–∏–¥', '—Ä—É–±–ª', '–¥–æ–ª–ª–∞—Ä', '–Ω–µ—Ñ—Ç', '–≥–∞–∑', 
                                                         '—Å–±–µ—Ä–±–∞–Ω–∫', '–≤—Ç–±', '–≥–∞–∑–ø—Ä–æ–º', '—ç–∫–æ–Ω–æ–º–∏–∫', '—Ä—ã–Ω–æ–∫', '–∏–Ω–≤–µ—Å—Ç']
                                    
                                    if any(keyword in text for keyword in financial_keywords):
                                        articles.append({
                                            'id': f"rbc_{pub_date_elem.text if pub_date_elem else ''}_{len(articles)}",
                                            'source': 'RBC',
                                            'title': title,
                                            'description': description,
                                            'content': description,
                                            'url': link_elem.text if link_elem is not None else '',
                                            'published_at': pub_date_elem.text if pub_date_elem is not None else '',
                                            'source_name': '–†–ë–ö',
                                            'fetched_at': datetime.now().isoformat()
                                        })
            
            logger.info(f"‚úÖ RBC RSS: –ø–æ–ª—É—á–µ–Ω–æ {len(articles)} —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ RBC RSS: {e}")
            return []
    
    def _deduplicate_news(self, all_articles: List[Dict]) -> List[Dict]:
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É"""
        seen_titles = set()
        unique_articles = []
        
        for article in all_articles:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            title = article.get('title', '').strip().lower()
            title_key = title[:80]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é —á–∞—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∞
            
            if title_key and title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_articles.append(article)
        
        removed = len(all_articles) - len(unique_articles)
        if removed > 0:
            logger.info(f"üîÑ –£–¥–∞–ª–µ–Ω–æ {removed} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, –æ—Å—Ç–∞–ª–æ—Å—å {len(unique_articles)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö")
        
        return unique_articles
    
    async def fetch_all_news(self) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –í–°–ï–• –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
        logger.info("üì• –ù–∞—á–∏–Ω–∞—é —Å–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –í–°–ï –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
        tasks = [
            self.fetch_mediastack(),  # –û—Å–Ω–æ–≤–Ω–æ–π
            self.fetch_zenserp(),     # –†–µ–∑–µ—Ä–≤–Ω—ã–π
            self.fetch_moex_rss(),    # –° –±–∏—Ä–∂–∏
            self.fetch_rbc_rss()      # –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π
        ]
        
        results = await asyncio.gather(
            *tasks,
            return_exceptions=True  # –ï—Å–ª–∏ –æ–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫ —É–ø–∞–ª - –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ä–∞–±–æ—Ç–∞—é—Ç
        )
        
        # –°–æ–±–∏—Ä–∞–µ–º —É—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        all_articles = []
        source_names = ['Mediastack', 'Zenserp', 'MOEX', 'RBC']
        
        for i, result in enumerate(results):
            if isinstance(result, list):
                all_articles.extend(result)
                logger.info(f"   üìä {source_names[i]}: {len(result)} –Ω–æ–≤–æ—Å—Ç–µ–π")
            elif isinstance(result, Exception):
                logger.error(f"   ‚ùå {source_names[i]} —É–ø–∞–ª: {str(result)[:50]}")
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        unique_articles = self._deduplicate_news(all_articles)
        
        logger.info(f"üìä –ò–¢–û–ì–û: {len(unique_articles)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–Ω–æ–≤—ã–µ —Å–Ω–∞—á–∞–ª–∞)
        unique_articles.sort(
            key=lambda x: x.get('published_at', ''),
            reverse=True
        )
        
        return unique_articles[:15]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 15 –Ω–æ–≤–æ—Å—Ç—è–º–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    
    def get_source_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º"""
        return {
            'mediastack_configured': bool(self.mediastack_key),
            'zenserp_configured': bool(self.zenserp_key),
            'moex_feeds': len(self.moex_feeds),
            'rbc_feeds': len(self.rbc_feeds),
            'cache_size': len(self.news_cache)
        }
